# End-to-End-Data-Engineering-on-Databricks


This is a comprehensive project that brings together key concepts and tools covered in the **Databricks Data Engineer Associate** certification and real-world data engineering workflows using the **Databricks Lakehouse Platform**.

It demonstrates practical experience in:

- Unity Catalog
- Delta Lake
- Delta Live Tables (DLT)
- Spark Structured Streaming
- Databricks Jobs
- Git integration with Databricks Repos
- ADLS Gen2 integration using External & Managed Locations
- Python & SQL-based expectations and SCD handling

---

##  Project Overview

### ðŸ“‚ Data Flow
Raw source data (CSV/JSON) from **ADLS Gen2** is ingested into:

- **Bronze** layer using Autoloader and streaming
- **Silver** layer using cleaning rules (DLT expectations)
- **Gold** layer using business logic (aggregations, deduplication, joins)

All pipelines are fully orchestrated using **DLT pipelines** and **Databricks Jobs**.

---

## ðŸ”§ Tools & Technologies

- **Platform**: Azure Databricks (Free Trial)
- **Storage**: ADLS Gen2 via External Volumes
- **Ingestion**: Autoloader (cloudFiles)
- **Processing**: Spark + PySpark
- **Table Format**: Delta Lake
- **Pipelines**: Delta Live Tables (DLT)
- **Governance**: Unity Catalog
- **Automation**: Databricks Jobs
- **Version Control**: Git (via Databricks Repos)

---

## ðŸ“Œ Key Concepts Practiced

| Topic                      | Covered |
|---------------------------|---------|
| Delta Lake & Transaction Log | âœ…     |
| MERGE INTO & SCD Handling     | âœ…     |
| Streaming & Micro-Batching    | âœ…     |
| Checkpointing & Triggers     | âœ…     |
| Z-Ordering & Compaction      | âœ…     |
| Vacuum & Data Retention      | âœ…     |
| CREATE OR REFRESH STREAMING TABLE | âœ… |
| Unity Catalog: Managed vs External Locations | âœ… |
| External Volumes & ADLS Integration | âœ… |
| Structured Streaming to Delta Tables | âœ… |
| SQL Constraints in DLT       | âœ…     |

---

##  How to Reproduce

1. Set up **Unity Catalog**, **External Location**, and **Volumes**
2. Upload datasets to ADLS paths
3. Use the provided notebooks or DLT pipelines to ingest and clean data
4. Use `MERGE INTO`, expectations, and SCD handling for upserts
5. Query Delta tables in SQL or Python

---

##  What I Learned

- How to build a lakehouse architecture using real streaming pipelines
- Hands-on with schema enforcement, versioning, time travel, and ACID guarantees
- How to orchestrate streaming data jobs with DLT and Databricks Jobs
- Unity Catalogâ€™s role in secure, scalable governance

---
- Thanks to Ramesh Ratnasamy for this certification course on udemy!


